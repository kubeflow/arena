# Default values for tensorflow-serving.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

## Kubernetes configuration
## support NodePort, LoadBalancer
##
serviceType: NodePort

## expose the service to the grpc client
port: 9000
replicas: 1

# repository: "cheyang/tf-model-server-gpu"
image: "cheyang/tf-model-server:1.4"

imagePullPolicy: "IfNotPresent"

resources: {}
  #  limits:
  #    cpu: 1.0
  #    memory: 512Mi
  #    nvidia.com/gpu: 1
  #  requests:
  #    cpu: 1.0
  #    memory: 512Mi
  #    nvidia.com/gpu: 1


## The command and args to run the pod
modelName: "inception"
modelPath: "/serving/inception-export"

## the mount path inside the container
# mountPath: /serving/inception-export

persistence:
  enabled: true
  storageClass: ""
  size: 10Gi
  accessMode: ReadWriteOnce
  existingClaim: tf-serving-pvc
 # matchLabels: {}



