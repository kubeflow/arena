# Default values for tfjob.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

useHostNetwork: false
useHostPID: true
useHostIPC: true
gpuCount: 0 # user define
privileged: false

# device and resource
#devices: amd.com/gpu=1

chief: 0
# Possible value: Chief/Master
chiefName: Chief
# chiefPort: 2221
chiefAnnotations: {}
# chiefAnnotations:

workers: 1
workerImage: kubeflow/tf-dist-mnist-test:1.0
# workerCPU: 1
# workerMemory: 1 Gi
# workPort: 2222

ps: 0
psImage: kubeflow/tf-dist-mnist-test:1.0
# psCPU: 1
# psMemory: 1 Gi
# psGPU: 1
# psPort: 2223
annotations: {}

evaluator: 0
evaluatorImage: kubeflow/tf-dist-mnist-test:1.0
evaluatorAnnotations: {}

# rsync image
rsyncImage: registry.cn-zhangjiakou.aliyuncs.com/acs/rsync:v3.1.0-aliyun
# git sync image
gitImage: registry.cn-zhangjiakou.aliyuncs.com/acs/git-sync:v3.3.5

imagePullPolicy: Always

useTensorboard: false
tensorboardImage: registry.cn-zhangjiakou.aliyuncs.com/acs/tensorflow:1.12.0-devel
tensorboardImagePullpolicy: Always
tensorboardServiceType: NodePort
tensorboardResources: {}
# tensorboardResources:
#   limits:
#     cpu: 500m
#     memory: 500Mi
#   requests:
#     cpu: 500m
#     memory: 500Mi

trainingLogdir: /output/training_logs

# disable by default 
binpack: false

# enable gang scheduler
enableGangScheduler: false
gangSchedulerName: kube-batchd

# enable RDMA support
enableRDMA: false

ingress: false

# hostLogPath: /training_logs/112345_logs

# nvidiaPath: /usr/local/nvidia-docker/nvidia_driver/384.81

# enable PodSecurityContext
# In the future, this flag should be protected separately, in case of arena admin and users are not the same people
enablePodSecurityContext: false

# enable priorityClassName
priorityClassName: ""

# add pod group
podGroupName: ""
podGroupMinAvailable: "1"

# Specifies the policy to mark the TF job as successful.
successPolicy: ""
