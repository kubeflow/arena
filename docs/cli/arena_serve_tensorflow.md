## arena serve tensorflow

Submit tensorflow serving job to deploy and serve machine learning models.

### Synopsis

Submit tensorflow serving job to deploy and serve machine learning models.

```
arena serve tensorflow [flags]
```

### Options

```
  -a, --annotation stringArray     the annotations
      --command string             the command will inject to container's command.
      --cpu string                 the request cpu of each replica to run the serve.
  -d, --data stringArray           specify the trained models datasource to mount for serving, like <name_of_datasource>:<mount_point_on_job>
      --data-subpath-expr stringArray  specify the datasource subpath to mount to the job by expression, like <name_of_datasource>:<mount_subpath_expr>
      --data-dir stringArray       specify the trained models datasource on host to mount for serving, like <host_path>:<mount_point_on_job>
      --enable-istio               enable Istio for serving or not (disable Istio by default)
  -e, --env stringArray            the environment variables
      --expose-service             expose service using Istio gateway for external access or not (not expose by default)
      --gpumemory int              the limit GPU memory of each replica to run the serve.
      --gpus int                   the limit GPU count of each replica to run the serve.
  -h, --help                       help for tensorflow
      --image string               the docker image name of serving job (default "tensorflow/serving:latest")
      --image-pull-policy string   the policy to pull the image, and the default policy is IfNotPresent (default "IfNotPresent")
      --memory string              the request memory of each replica to run the serve.
      --model-name string          the model name for serving
      --model-path string          the model path for serving in the container
      --modelConfigFile string     Corresponding with --model_config_file in tensorflow serving
      --name string                the serving name
      --port int                   the port of tensorflow gRPC listening port (default 8500)
      --replicas int               the replicas number of the serve job. (default 1)
      --restfulPort int            the port of tensorflow RESTful listening port (default 8501)
      --selector stringArray       assigning jobs to some k8s particular nodes, usage: "--selector=key=value" or "--selector key=value"
      --toleration stringArray     tolerate some k8s nodes with taints,usage: "--toleration taint-key" or "--toleration all"
      --version string             the serving version
      --version-policy string      support latest, latest:N, specific:N, all
```

### Options inherited from parent commands

```
      --arena-namespace string   The namespace of arena system service, like tf-operator (default "arena-system")
      --config string            Path to a kube config. Only required if out-of-cluster
      --loglevel string          Set the logging level. One of: debug|info|warn|error (default "info")
  -n, --namespace string         the namespace of the job
      --pprof                    enable cpu profile
      --trace                    enable trace
```

### SEE ALSO

* [arena serve](arena_serve.md)	 - Serve a job.

###### Auto generated by spf13/cobra on 5-Mar-2021
